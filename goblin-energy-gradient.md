# Goblin Energy Gradient Model

We treat **energy** as effort or cost required to maintain *truth alignment* between declared models and runtime reality.

Let:

- \( x_i \) = normalized value of dimension \( i \) (e.g., distributed state, asynchrony, observability deficit).
- \( w_i \) = weight of dimension \( i \) in the goblin score.
- Goblin score \( G \in [0,1] \).

## Goblin Score (re-stated)

We define:

\[
G = \sum_i w_i x_i
\]

with \( \sum_i w_i = 1 \), \( 0 \leq x_i \leq 1 \).

This is a linear functional over the dimension space capturing:

- Illusion of closure
- Distributed state
- Asynchrony
- Stakeholder demands for guarantees
- Observability deficit
- Energy gradient misalignment

## Energy Gradient

Define an "alignment energy" \( E \) such that higher goblin score means higher latent misalignment energy:

\[
E = k \cdot G
\]

for some constant \( k > 0 \) (scale factor in arbitrary units, e.g., "risk units" or "ops burn").

### Gradient of E

In the dimension space:

\[
\nabla E = k \cdot \nabla G = k \cdot (w_1, w_2, \dots, w_n)
\]

This gradient indicates **direction of steepest increase** in misalignment energy as you move along dimensions \( x_i \).

- Large \( w_i \) means that reducing \( x_i \) (e.g., improving observability) most effectively reduces \( E \).

### Local Optimization vs Global Alignment

Agents (developers, teams, org units) tend to follow **local energy gradients** that minimize *their own* effort, which is not the same as minimizing \( E \).

We can separate:

- \( \nabla E_{\text{global}} \): gradient for global misalignment energy.
- \( \nabla E_{\text{local, role}} \): gradient for a given stakeholder role's perceived cost/benefit.

Goblin-heavy systems often have:

\[
\nabla E_{\text{local, role}} \cdot \nabla E_{\text{global}} < 0
\]

for key roles, meaning:

> Local "easy" moves make global misalignment worse.

This is the **energy gradient misalignment** component that drives Goblin scores upward.

## Flow Interpretation

We can treat Goblin remediation as a gradient descent problem:

\[
x_i^{(t+1)} = x_i^{(t)} - \eta \frac{\partial E}{\partial x_i}
           = x_i^{(t)} - \eta k w_i
\]

where \( \eta \) is a learning rate representing investment level in systemic quality.

In practice:

- Increasing observability (reducing \( x_{\text{obs-deficit}} \)) and
- Reducing illusion of closure (reducing \( x_{\text{illusion}} \))

are high-leverage moves when their \( w_i \) are large.

Goblin score gives you a way to **prioritize** where to push on the system to reduce long-term misalignment energy.

## Numeric scenarios

Using the default weight assignment (`ui:DefaultWeights`):

- \( w_{\text{illusion}} = 0.18\)
- \( w_{\text{distributed}} = 0.20\)
- \( w_{\text{async}} = 0.15\)
- \( w_{\text{stakeholder}} = 0.10\)
- \( w_{\text{observability}} = 0.12\)
- \( w_{\text{energy-misalignment}} = 0.25\)

### Scenario A: Targeted fix (observability + invariants)

Before: \( x = (0.90, 0.80, 0.80, 0.60, 0.70, 0.90) \) ⇒ \( G \approx 0.81 \)

After investing in domain invariants and observability: \( x = (0.80, 0.78, 0.78, 0.40, 0.70, 0.88) \) ⇒ \( G \approx 0.72 \).

\(\Delta G \approx -0.09 \) (~10% reduction). With \( k = 1 \), \( \Delta E = \Delta G \).

### Scenario B: Local optimization that backfires

Local move: reduce latency by caching and suppressing validation → \( x = (0.95, 0.80, 0.70, 0.75, 0.70, 0.90) \) ⇒ \( G \approx 0.83 \).

Even though asynchrony improves, closure illusions and domain drift increase the overall score.

Tie-in to SHACL:
- The shapes constrain values like `ui:goblinScore` and ensure components (e.g., `ui:componentValue`) stay in \([0,1]\).
- Scenarios can be serialized as RDF/JSON-LD and validated against `goblin-shapes.ttl` to keep data consistent while experimenting.
